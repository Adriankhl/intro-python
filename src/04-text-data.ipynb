{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c01da5",
   "metadata": {},
   "source": [
    "## String basics\n",
    "Strings in Python come with a number of useful features and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfox = \"the quick brown fox jumped over the lazy dog. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3669c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfox.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093fd06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfox.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'fox' in quickfox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0796036",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfox.startswith('fox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba341ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfox.find(\"fox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f911d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfox[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfox.count('fox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc284a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfox.replace('fox', 'hare').replace('lazy', 'adorable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a032a3",
   "metadata": {},
   "source": [
    "Splitting strings is an important standard operation that allows you to produce lists of substrings, based on a defined separator. In this case, we split the sentence by whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142210af",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfox.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9183af",
   "metadata": {},
   "source": [
    "Note the empty string at the end of the list. This exists because the original string ended in a whitespace. We can use the .strip() method to remove leading and trailing whitespace from a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quickfox.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46822186",
   "metadata": {},
   "source": [
    ".join() is a powerful method that allows you to join a list of strings together, using the specified separator. In this case, we will join a list of numbers together, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60da621",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_list = ['one', 'two', 'three', 'four']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "' and '.join(example_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a9b26",
   "metadata": {},
   "source": [
    "## Pandas stringtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785012ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/publications.txt', sep='\\t', encoding='utf-8', dtype={'authors': 'string', 'journal_title': 'string', 'paper_title': 'string', 'abstract': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['paper_title'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['paper_title'].str.find('citation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67652df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['authors'].str.split('; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['authors'].str.contains('van Eck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['authors'].str.contains('van Eck')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5f46",
   "metadata": {},
   "source": [
    "## Formatted strings\n",
    "Allow for insertion of variables and even expressions within the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "name='Wout'\n",
    "f'My name is {name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "a='Amsterdam'\n",
    "b='the Netherlands'\n",
    "c=800000\n",
    "f'{a} is the capital of {b} and it has a population of over {c}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "b = '5'\n",
    "c = 10\n",
    "f'{a} times {c} is {a*c} but {b} times {c} is {b*c}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea9494",
   "metadata": {},
   "source": [
    "Beware - typically ' and \" do not mix, though either can be used to define strings. If you use strings within the expressions in an f-string, you will have to use a different style, else you get a syntax error, as in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'The dataframe contains {df['authors'].str.contains('van Eck').sum()} articles by Nees Jan van Eck.' # this returns an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"The dataframe contains {df['authors'].str.contains('van Eck').sum()} articles by Nees Jan van Eck.\" # this works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b2b2d",
   "metadata": {},
   "source": [
    "## Regular expression\n",
    "A powerful tool for parsing and editing string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efde2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ceb02",
   "metadata": {},
   "source": [
    "Let's start by retrieving the abstract of Vincent's paper in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "vincent_abstract = df.loc[df['authors'].str.contains('Traag') & df['abstract'].notna()]['abstract'].tolist()[0]\n",
    "vincent_abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80d08a",
   "metadata": {},
   "source": [
    "Regular expressions allow you to quickly search and manipulate strings. It uses wildcards, patterns, quantifiers, and character groups. For instance, we can find any numeric character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53285a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('[0-9]', vincent_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392d3f3",
   "metadata": {},
   "source": [
    "Regex uses a number of special characters, such as parentheses and square brackets, to denote groups of characters. If you want to explicitly look for these, you need to escape them with a backslash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('\\([0-9]\\)', vincent_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1dabf3",
   "metadata": {},
   "source": [
    "Quantifiers can be used to denote numbers of characters to look for. Let's find any substring that consists of at least two capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3084bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('[A-Z]{2,}', vincent_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9d072",
   "metadata": {},
   "source": [
    "Finally, let's use wildcards to match any character between the numbers in parentheses, and ending at the first semicolon or period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('\\([0-9]\\).*?[;.]', vincent_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f53963",
   "metadata": {},
   "source": [
    "Regex allows for more than just finding or matching patterns. It can also be used to substitute a pattern with a new string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub('\\([0-9]\\).*?[;.]', '<SENTENCE REMOVED>', vincent_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55315fb",
   "metadata": {},
   "source": [
    "Other important built-in features are the detection of the start of a string (^) and the end of a string (%). We can, for instance, extract the first sentence of the abstract by searching for a pattern, starting from the start of the string, up until the first period. re.search returns a match object, which contains both the matched text as well as the location in the original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5631a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search('^.*?\\.', vincent_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d291117",
   "metadata": {},
   "source": [
    "There are many more things that you can do with regular expression, which we will not get into today, as it gets rather complex very fast. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336911a",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "So far, most of these string operations have been possible within SQL as well, so you migth be asking, why Python? The Natural Language ToolKit is the first of a large list of libraries that allow you to do much more with text data than before. First, we need to download the nltk corpus and model files. Run the below cell, then download the 'popular' packages, that is enough for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec47f0",
   "metadata": {},
   "source": [
    "When working with longer texts, it is often useful to break them up into individual sentences, or even words. This is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe03a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vincent_sentences = sent_tokenize(vincent_abstract)\n",
    "vincent_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f10f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the abstract into individual words\n",
    "vincent_words = word_tokenize(vincent_abstract)\n",
    "vincent_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543b490",
   "metadata": {},
   "source": [
    "Note that there are a lot of 'stopwords' in sentences. These typically add little to a quantitative analysis of text, and can be removed. NLTK has lists of stopwords for various languages. Let's remove these from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da926ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vincent_words = [w.lower() for w in vincent_words if w.lower() not in stopwords.words(\"english\")]\n",
    "print(vincent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae9134",
   "metadata": {},
   "source": [
    "let's also remove all tokens that consists of non-alphabetical characters, with a simple regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aba66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vincent_words = [w for w in vincent_words if bool(re.match('[^a-z]', w))==False]\n",
    "print(vincent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09b0d1",
   "metadata": {},
   "source": [
    "### Lemmatizataion and stemming\n",
    "Stemming reduces words to a base stem form by using predefined rules to trim the endings of nouns and verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "for w in vincent_words:\n",
    "    stemmed = ps.stem(w)\n",
    "    if w != stemmed:\n",
    "        print(w, \" : \", stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f231b",
   "metadata": {},
   "source": [
    "Lemmatization looks up words and replaces them with their base form, if found. The downside is that unknown words are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e634b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in vincent_words:\n",
    "    lemmed = WordNetLemmatizer().lemmatize(w)\n",
    "    if w != lemmed:\n",
    "        print(w, \" : \", lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a7ee8",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "We can find part-of-speech tags (nouns, verbs, etc) using NLTK, as well. This allows us to extract, for instance, all verbs from Vincent's abstract. First, let's return to the original tokenized word list, then tag them sentence by sentence. See https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html for a list of all POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca756524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "sent_pos_tags = [pos_tag(word_tokenize(sent)) for sent in vincent_sentences]\n",
    "print(sent_pos_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve verbs\n",
    "[[v[0] for v in s if v[1][0]=='V'] for s in sent_pos_tags]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
