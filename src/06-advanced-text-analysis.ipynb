{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b676cd9b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this session, I will introduce you to the basics of natural language processing in Python. We will touch on several approaches and models, though we will not go too in-depth. This tutorial builds on the basics of text data introduced in tutorial session 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a5cf9",
   "metadata": {},
   "source": [
    "## Load data\n",
    "First, let's retrieve the publication data we have been using in this tutorial series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/publications.txt', sep='\\t', encoding='utf-8', dtype={'authors': 'string', 'journal_title': 'string', 'paper_title': 'string', 'abstract': 'string'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e86e7d",
   "metadata": {},
   "source": [
    "We need text to process, so we delete any publication without an abstract. We will only be using abstracts for this excersise, but you could easily include paper titles, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69fa478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['abstract'].notna()].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b0dc86",
   "metadata": {},
   "source": [
    "Let's also fetch the abstract from Vincent's publication that we used in tutorial 4, so we have a clear example to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vincent_pub = df.loc[df['authors'].str.contains('Traag')]\n",
    "vincent_abstract = vincent_pub['abstract'].tolist()[0]\n",
    "vincent_abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf147a",
   "metadata": {},
   "source": [
    "# Text vectorization\n",
    "Many natural language processing techniques rely on the ability to represent documents as series of numbers, or a vector. These vectors can then be compared to each other, for instance, one can compare the directions they point in to determine if the original documents contain similar words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b4615e",
   "metadata": {},
   "source": [
    "## Bag-of-words models\n",
    "A common approach in natural language processing is to treat text as simply a collection (bag) of words - ignoring their order, meaning, or grammatical structures within the text. Each word in the set of texts (the corpus) is treated as a column of data, or a dimension in a vector space, on which texts are then scored.\n",
    "\n",
    "The most straightforward way of doing so is simply to count the number of times each word occurs in a document.\n",
    "\n",
    "Remember the end of tutorial 4, in which we split Vincent's abstract into indivudal tokens, and removed stop-words and non-word tokens from the list. We repeat the same process here. If you have not attended tutorial 4, first run the cell below and download the \"popular\" packages."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2520030",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f25e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "vincent_words = word_tokenize(vincent_abstract)\n",
    "vincent_words = [w.lower() for w in vincent_words if w.lower() not in stopwords.words(\"english\")]\n",
    "vincent_words = [w for w in vincent_words if bool(re.search('[^a-z]', w))==False]\n",
    "vincent_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e8eae",
   "metadata": {},
   "source": [
    "Some of these terms are clearly permutations of others. The plural 'countries', for instance, can be rendered instead as 'country' because the meaning of the two terms is identical (if not close enough to make no difference). This is where lemmatization comes into play again. We use WordNetLemmatizer to simplify our terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17819066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "vincent_words = [WordNetLemmatizer().lemmatize(w) for w in vincent_words]\n",
    "vincent_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d5b82",
   "metadata": {},
   "source": [
    "Now all that remains is to count the number of times each remaining term occurs in the text. `Counter` is a useful little class that does exactly this. Let's also order the words from most-common to least-common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(vincent_words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2d7b9",
   "metadata": {},
   "source": [
    "We have now successfully reduced Vincent's abstract to just a bag of words, completely ignoring any subtlety or grammar in the original text. But, this new list is far more easy for a computer to interpret.\n",
    "\n",
    "Of course, we can repeat the same process for our entire corpus. Let's write a quick class that combines the tokenization and lemmatization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98866122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        text = word_tokenize(text.lower())\n",
    "        text = [w for w in text if bool(re.search('[^a-z]', w))==False]\n",
    "        return [self.wnl.lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65256542",
   "metadata": {},
   "source": [
    "One of the reasons Python is such a useful tool is that many libraries exist that do the hard work for you. In our case, `sklearn` includes a handy function called `CountVectorizer` that can quickly apply the cleaning and tokenization steps and then vectorize our entire corpus. This produces a sparse matrix object, in which each row represents a document and each column represents a term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bf350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                             strip_accents = 'unicode',\n",
    "                             stop_words = 'english')\n",
    "X = tf_vectorizer.fit_transform(df['abstract'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058e9ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    How many unique terms are in our resulting model?\n",
    "</div>\n",
    "\n",
    "[//]: # \"X.shape[1]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ddd42",
   "metadata": {},
   "source": [
    "The matrix does not store the actual terms themselves, only the data contents. Documents (rows) are in the original order, while terms are listed alphabetically. The `vectorizer` object stores the terms that were extracted from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tf_vectorizer.get_feature_names_out().tolist()\n",
    "terms[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0ac1f",
   "metadata": {},
   "source": [
    "The complete vocabulary is stored in a dictionary, along with indices for each term. For instance, here's how to find the index for `'science'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3335853",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=tf_vectorizer.vocabulary_\n",
    "vocab['science']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3248b80d",
   "metadata": {},
   "source": [
    "We can use this index to retrieve the number of times the word `'science'` features in the documents of our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,vocab['science']].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150ea47e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Which publication mentions the term <code>'science'</code> most often?\n",
    "</div>\n",
    "\n",
    "[//]: # \"print(f'The term science occurs a maximum of {X[:,vocab['science']].max()} times at index {X[:,vocab['science']].argmax()}:')\"\n",
    "[//]: # \"df.iloc[[X[:,vocab['science']].argmax()]]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The term 'science' occurs a maximum of {X[:,vocab['science']].max()} times at index {X[:,vocab['science']].argmax()}:\")\n",
    "df.iloc[[X[:,vocab['science']].argmax()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc32c9b",
   "metadata": {},
   "source": [
    "But, why go through all this trouble? What is this good for?\n",
    "\n",
    "Well, we can use these vectors to compare documents. For instance, if we take the angle between these vectors, we can see how similar they are with regards to the words they use. An often-used measure for this is cosine similarity, or cosine distance.\n",
    "\n",
    "Let's see which abstract is most similar to Vincent's, according to this measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a32400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_scores = cosine_similarity(X[vincent_pub.index], X)[0]\n",
    "\n",
    "# we're not interested in Vincent's paper's similarity to itself, so let's set that to 0\n",
    "similarity_scores[vincent_pub.index] = 0\n",
    "\n",
    "print(f'paper {similarity_scores.argmax()} with cosine similarity {similarity_scores.max()}')\n",
    "\n",
    "df.iloc[[similarity_scores.argmax()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b584462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[similarity_scores.argmax()]]['abstract'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e6633",
   "metadata": {},
   "source": [
    "## Term weighing\n",
    "You do not have to use raw term counts. Sometimes it makes sense to weigh term occurrences to prioritize or reduce the importance of certain terms. After all, in natural language, common terms are *far* more common than rare terms (see Zipf's law), but those very common terms are hardly ever the most interesting ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7128701",
   "metadata": {},
   "source": [
    "### Logarithmic scaling\n",
    "Some researchers prefer to use logarithmic weighing for reducing the importance of overly-common terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_ln = X.copy()\n",
    "X_ln.data = np.log(X_ln.data+1) # add 1 to avoid taking the log of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list each term, its raw count, and its logarithmic weight\n",
    "[(terms[t], X[vincent_pub.index[0],t], round(X_ln[vincent_pub.index[0],t], 3)) for d,t in zip(*X[vincent_pub.index].nonzero())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ad9c9",
   "metadata": {},
   "source": [
    "### Tf-idf\n",
    "Another popular way of weighing terms is to use tf-idf. This is the term frequency `tf` (raw term counts divided by total number of terms in the document - a measure of term importance in the document) multiplied by the inverse document frequency `idf` (logarithm of the number of documents divided by the number of documents containging the term - a measure of term specificity in the corpus).\n",
    "\n",
    "tf-idf is a very practical way of weighing terms, even if it isn't very theoretically informed. Like `CountVectorizer`, there is a convenient `TfidfVectorizer` function we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), \n",
    "                                   strip_accents = 'unicode',\n",
    "                                   stop_words = 'english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['abstract'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f37503",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(terms[t], X[vincent_pub.index[0],t], round(X_tfidf[vincent_pub.index[0],t], 3)) for d,t in zip(*X[vincent_pub.index].nonzero())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf9c89",
   "metadata": {},
   "source": [
    "Note that terms with identical counts in Vincent's abstract now have different tf-idf scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138aff4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Judging just by these scores, which of the three terms <code>research</code>, <code>uk</code> and <code>uncertainty</code> occurs in most documents in the corpus?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a86660",
   "metadata": {},
   "source": [
    "One of the main benefits of tf-idf is that its main feature, the penalizing of terms that are ubiquitous throughout the corpus, allows us to get away with less pre-processing of the data. Stop-words are common words by definition, and will be heavily penalized by tf-idf, reducing their scores across the board. This means that it makes little difference whether you remove stop-words or not when using tf-idf, and that common words that are not on any stop-word lists are similarly reduced in importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523377b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Using cosine similarity of <code>tf-idf</code> instead of raw term counts, which paper abstract is most similar to Vincent's abstract?\n",
    "</div>\n",
    "\n",
    "[//]: # \"similarity_scores_tfidf = cosine_similarity(X_tfidf[vincent_pub.index], X_tfidf)[0]\"\n",
    "\n",
    "[//]: # \"# we're not interested in Vincent's paper's similarity to itself, so let's set that to 0\"\n",
    "[//]: # \"similarity_scores_tfidf[vincent_pub.index] = 0\"\n",
    "\n",
    "[//]: # \"print(f'paper {similarity_scores_tfidf.argmax()} with cosine similarity {similarity_scores_tfidf.max()}')\"\n",
    "\n",
    "[//]: # \"df.iloc[[similarity_scores_tfidf.argmax()]]\"\n",
    "\n",
    "[//]: # \"df.iloc[[similarity_scores_tfidf.argmax()]]['abstract'].tolist()[0]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45480dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores_tfidf = cosine_similarity(X_tfidf[vincent_pub.index], X_tfidf)[0]\n",
    "\n",
    "# we're not interested in Vincent's paper's similarity to itself, so let's set that to 0\n",
    "similarity_scores_tfidf[vincent_pub.index] = 0\n",
    "\n",
    "print(f'paper {similarity_scores_tfidf.argmax()} with cosine similarity {similarity_scores_tfidf.max()}')\n",
    "print(df.iloc[[similarity_scores_tfidf.argmax()]]['abstract'].tolist()[0])\n",
    "df.iloc[[similarity_scores_tfidf.argmax()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b7575",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "One of the main downsides of bag-of-words approaches, weighted or not, is that the length of your vectors depends on the number of unique terms in your corpus after preprocessing. For large corpora, this can quickly become burdensome. Moreover, terms can mean similar things, but never occur next to each other. Imagine if one paper mentiones `'tf-idf'` in its abstract, while another only ever mentions it without the dash, `'tfidf'`. If we look purely at term co-occurrence in these two documents, `'tf-idf'` and `'tfidf'` appear unrelated, while they stand for the same thing.\n",
    "\n",
    "Dimensionality reduction can help alleviate both these issues, through a variety of techniques. Some find the principal components of the vector space, others try to find latent structures, while yet others use neural networks to optimize text prediction problems. In essence though, what all these techniques have in common is that they attempt to reduce the dimensionality of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db86b7de",
   "metadata": {},
   "source": [
    "### Topic modeling with LDA\n",
    "A topic model is a statistical model used to discover abstract semantic structures in a corpus. It uses the bag-of-words representation of documents in a corpus to find latent \"topics\" - essentially, distributions - from which documents draw their terms. Very much simplified, each \"topic\" contains a distribution of different terms, and the assumption is that documents draw terms from a limited number of topics. Optimizing these distributions for a given number of topics then results in your topic model.\n",
    "\n",
    "Latent Dirichlet allocation (LDA) is the most popular approach for this, though multiple alternatives exists. Typically, the user has to specify the number of topics they want to find, after which their preferred topic modeling approach tries to determine which distributions of terms over topics are optimal.\n",
    "\n",
    "Gensim is a useful library for topic modeling, though it uses its own corpus files. Let's try and find five topics in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's make our corpus smaller. There are some non-english publications in our set, and they are causing trouble.\n",
    "get_journals = df['journal_title'].str.contains('Scientometrics|Journal of Psychology of Science and Technology|Journal of Information Science')\n",
    "get_journals = df['journal_title'].str.contains('Scientometrics|Journal of Information Science')\n",
    "get_authors = df['authors'].str.contains('Traag|van Eck|Waltman')\n",
    "df_small = df.copy().loc[get_journals|get_authors].reset_index(drop=True)\n",
    "vincent_pub_s = df_small.loc[df_small['authors'].str.contains('Traag')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63620d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                             strip_accents = 'unicode',\n",
    "                             stop_words = 'english')\n",
    "X_small = tf_vectorizer.fit_transform(df_small['abstract'].tolist())\n",
    "vocab = tf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f351a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "corpus = matutils.Sparse2Corpus(X_small)\n",
    "lda = LdaModel(corpus, num_topics=5, id2word={i: w for w, i in vocab.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad733e",
   "metadata": {},
   "source": [
    "We can print the resultant 'topics' by their most relevant (highest-probability) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics(num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47f5bf",
   "metadata": {},
   "source": [
    "Note that these probabilities are topic->word estimates, so the sum of probabilities of words over a topic equals one. Similarly, if we pass a word vector to the LDA model, it will return a distribution of topics (summing to one) that it thinks this word vector is comprised of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a36ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vincent_vector = [[i, X_small[vincent_pub_s.index[0],i]] for i in X_small[vincent_pub_s.index].nonzero()[1]]\n",
    "lda.get_document_topics(vincent_vector, minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb41bc5",
   "metadata": {},
   "source": [
    "Note that if we retrieve topics for terms, these do not sum to one, as they report the probability the term is drawn from each topic, not the distribution of topics over terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.get_term_topics(vocab['collaboration'], minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0395a1",
   "metadata": {},
   "source": [
    "While topics generated by topic modeling are supposedly human-interpretable, this is not always easy or straightforward. In any case, we have now reduced our several thousands of terms to five topics - a large step down in dimensionality. In this case though, I would not call it an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c6f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference, an attempt at running cosine similarity on Vincent's abstract's topic distribution vs the other abstract in the set:\n",
    "all_docs = [[[i, X_small[d,i]] for i in X_small[d].nonzero()[1]] for d in range(X_small.shape[0])]\n",
    "all_docs = [lda.get_document_topics(w, minimum_probability=0) for w in all_docs]\n",
    "all_docs = [[i[1] for i in w] for w in all_docs]\n",
    "\n",
    "similarity_scores_lda = cosine_similarity([all_docs[vincent_pub_s.index.tolist()[0]]], all_docs)[0]\n",
    "similarity_scores_lda[vincent_pub_s.index.tolist()[0]] = 0\n",
    "\n",
    "print(f'paper {similarity_scores_lda.argmax()} with cosine similarity {similarity_scores_lda.max()}')\n",
    "print(df_small.iloc[[similarity_scores_lda.argmax()]]['abstract'].tolist()[0])\n",
    "df_small.iloc[[similarity_scores_lda.argmax()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393cc97",
   "metadata": {},
   "source": [
    "## Word & document embedding\n",
    "So far, all the aproaches to natural language processing we tried have treated text as simply an unordered collection of words, completely ignoring any kinds of grammar or semantic structure. Word embeddings, and subsequent approaches, try to improve on this by using words' neighbors in their representation. It does so by training a shallow neural network to predict words from their context, or context from given words. After training, the resulting neuron weights can function as a vector space. \n",
    "\n",
    "Doc2vec is a technique based on this, which includes the ability to tag documents along with their contents, and also generate embeddings for these tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97975ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e280b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = LemmaTokenizer()\n",
    "documents = [TaggedDocument(tokenize(abs), [str(i)]) for i, abs in df['abstract'].iteritems()]\n",
    "#documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "d2v = Doc2Vec(documents, vector_size=100, window=5, min_count=1, workers=1)\n",
    "# set workers to a higher number if you have multiple threads available, or keep at 1 for reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82aa24",
   "metadata": {},
   "source": [
    "Word embedding methods return embedding vectors of a specified size, in the example above I have chosen 100. The window size is the maximum distance around a word that its neighbors are selected from, and those neighbors are then used to predict the relevant word.\n",
    "\n",
    "Word2vec and doc2vec actually have two training modes - Continuous Bag Of Words which uses neighbors to predict a word, and skip-gram/distributed memory, which uses a word to predict its neighbors. CBOW is more intuitive, but I find skip-gram produces better results for infrequent terms, which we deal with often in science.\n",
    "\n",
    "Unlike topic modeling, which claims to produce \"human interpretable\" dimensions, embedding dimensions themselves are meaningless. It is the combination of these, and the relationships between embedded terms, that matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the embedding for a word\n",
    "d2v.wv['science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18299ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the embedding for a document\n",
    "d2v.dv[str(vincent_pub.index[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df417dae",
   "metadata": {},
   "source": [
    "Embeddings allow one to compare the embedded terms themselves. For instance, we can easily find the terms most similar to one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83056aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v.wv.most_similar('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce0be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which documents are most similar to Vincent's abstract?\n",
    "most_similar=[i for i in d2v.dv.most_similar(str(vincent_pub.index[0]))]\n",
    "i, s = most_similar[0]\n",
    "i = int(i)\n",
    "print(f'document {i} is most similar with a cosine of {s}')\n",
    "print(df.iloc[i]['abstract'])\n",
    "df.iloc[[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6717551",
   "metadata": {},
   "source": [
    "## Language models\n",
    "The downside of embedding models is that they are limited to the data you provide to them. Language models, on the other hand, are pre-trained and encapsulate some understanding of language. These, in turn, can be used to generate vector representations of terms and documents. Many different language models are available. We will focus on SciBERT, a variant of BERT trained on scientific text.\n",
    "\n",
    "Note: the following code downloads the SciBERT language model - its size is approx. half a gigabite.\n",
    "\n",
    "The sentence transformer we use allows us to embed larger texts in one go, but it is rather resource intensive. That's why, for this tutorial, I will further limit our corpus to just publications by Nees, Ludo and Vincent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models\n",
    "scibert_model = models.Transformer('allenai/scibert_scivocab_uncased')\n",
    "pooling_model = models.Pooling(scibert_model.get_word_embedding_dimension(),\n",
    "                            pooling_mode_mean_tokens=True,\n",
    "                            pooling_mode_cls_token=False,\n",
    "                            pooling_mode_max_tokens=False)\n",
    "sbert = SentenceTransformer(modules=[scibert_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = LemmaTokenizer()\n",
    "df_smallest = df[get_authors].reset_index(drop=True)\n",
    "documents = df_smallest['abstract'].tolist()\n",
    "documents = [' '.join(tokenize(d)) for d in documents]\n",
    "documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2385e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = sbert.encode(documents, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "vincent_index = df_smallest.loc[df_smallest['authors'].str.contains('Traag')].index[0]\n",
    "sentence_embeddings[vincent_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa57be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores_scibert = cosine_similarity([sentence_embeddings[vincent_index]], sentence_embeddings)[0]\n",
    "\n",
    "# we're not interested in Vincent's paper's similarity to itself, so let's set that to 0\n",
    "similarity_scores_scibert[vincent_index] = 0\n",
    "\n",
    "print(f'paper {similarity_scores_scibert.argmax()} with cosine similarity {similarity_scores_scibert.max()}')\n",
    "print(df.iloc[[similarity_scores_scibert.argmax()]]['abstract'].tolist()[0])\n",
    "df_smallest.iloc[[similarity_scores_scibert.argmax()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f65de8d",
   "metadata": {},
   "source": [
    "This might not look like a great match, but remember that we have rather aggressively limited our corpus in an earlier step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090734c",
   "metadata": {},
   "source": [
    "### Universal Sentence Encoder, for the truly adventurous\n",
    "SentenceTransformers and any BERT-based embedding have a rather important limitation, and that is that the length of the paragraphs they can embed is limited to 512 words. Google's new Universal Sentence Encoder is capable of embedding larger texts, but to run it, we need tensorflow, which I never managed to install correctly through anaconda. We can instead use pip. NOTE: Again, this is a very large library, and the model we download later on is likewise large.\n",
    "\n",
    "Close this notebook, and in anaconda prompt, type the following:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9786cd49",
   "metadata": {},
   "source": [
    "pip install tensorflow tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc2c51",
   "metadata": {},
   "source": [
    "tensorflow-hub allows us to easily fetch pre-trained models from the web and use them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as tfhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfhub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a641f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_embeddings = model(df_small['abstract'].tolist()).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e044af9f",
   "metadata": {},
   "source": [
    "Somewhat ironically, it seems that the more recent, advanced and complex a model is, the simpler it looks when you call it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23485101",
   "metadata": {},
   "outputs": [],
   "source": [
    "vincent_index = df_small.loc[df_small['authors'].str.contains('Traag')].index[0]\n",
    "similarity_scores_USE = cosine_similarity([abstract_embeddings[vincent_index]], abstract_embeddings)[0]\n",
    "\n",
    "# we're not interested in Vincent's paper's similarity to itself, so let's set that to 0\n",
    "similarity_scores_USE[vincent_index] = 0\n",
    "\n",
    "print(f'paper {similarity_scores_USE.argmax()} with cosine similarity {similarity_scores_USE.max()}')\n",
    "print(df.iloc[[similarity_scores_USE.argmax()]]['abstract'].tolist()[0])\n",
    "df_small.iloc[[similarity_scores_USE.argmax()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cca5c2",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "I hope this overview has been useful. I have focused exclusively on text similarity in this tutorial, while often times, the vector representations we derive here are used for downstream tasks such as classification or sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
